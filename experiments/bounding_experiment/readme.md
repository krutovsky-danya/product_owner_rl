Проблема: оценки действий очень большие, от этого градиент может взрываться, а процесс обучения затрудняться.

Гипотеза: при нормировании наград нейросети алгоритм будет лучше обучаться, тем самым давать лучшие результаты.

Метрика: количество побед алгоритма увеличилось.

Был взят пайплайн обучения из предыдущего эксперимента (guidance_experiment).
Увеличено количество эпизодов обучения до тысячи пятисот.
В измененном пайплайне награды алгоритма были доделены на 100.
Пропорции и логика наград не изменены.

Оригинальный график наград был поделен на 100, чтобы графики стали одно масштаба.
Из графика ниже видно, что существенных изменений в обучении не произошло.

![image](bounding_rewards.png)

Количество побед на сто итераций:

Обычная система наград:

[16.  0. 13.  6. 11. 16.  7. 18.  7.  8.  6.  5. 11. 12. 16. 15.  8.  7.  5.  7.]

Нормированная система наград:

[16. 23. 19.  4. 20. 13. 11.  5.  6.  3. 16. 12. 12. 12. 13. 14.  6. 12. 14.  4.]

Для проверки нулевой гипотезы используем тест Хи квадрат.

После теста p-value составляло 0.040960

Таким образом на данных показано, что нормирование наград имеет статистическую значимость.
